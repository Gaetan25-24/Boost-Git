# ğŸ­ ModÃ¨le de Classification d'Ã‰motions avec TensorFlow et MobileNetV2

Ce projet utilise un modÃ¨le de deep learning basÃ© sur MobileNetV2 pour classer des images faciales selon des Ã©motions humaines : **joie, tristesse, colÃ¨re, peur, calme, Ã©tonnement**.

## ğŸ“‚ FonctionnalitÃ©s

- Extraction des features avec `MobileNetV2` prÃ©-entraÃ®nÃ© sur ImageNet.
- EntraÃ®nement dâ€™un modÃ¨le personnalisÃ© sur les embeddings extraits.
- Augmentation des donnÃ©es avec `ImageDataGenerator`.
- Sauvegarde du meilleur modÃ¨le via `ModelCheckpoint`.
- PrÃ©diction dâ€™Ã©motion sur une image.

## ğŸ“¦ Technologies utilisÃ©es

- Python
- TensorFlow / Keras
- NumPy
- PIL (Python Imaging Library)
- scikit-learn

## ğŸ—ï¸ Architecture du modÃ¨le

- EntrÃ©e : vecteurs d'embeddings (1280 dimensions)
- Denses : 512 â†’ 256 â†’ 128 â†’ 64 â†’ 32
- Sortie : couche Softmax avec 6 classes
- Fonctions d'activation : ReLU
- Optimiseur : Adam
- Perte : `sparse_categorical_crossentropy`

## PrÃ©cision sur la validation (val_accuracy) : 
GÃ©nÃ©ralement plus basse (par exemple, 20-50 %) en raison du faible nombre d'Ã©chantillons (4 pour la validation) 
et des labels alÃ©atoires, ce qui rend la validation instable et peu fiable.En raison des donnÃ©es fictives, les performances
du modÃ¨le (par exemple, accuracy ~80-100 %, val_accuracy ~20-50 %) ne sont pas reprÃ©sentatives. Avec un jeu de donnÃ©es rÃ©el et diversifiÃ©, on pourrait s'attendre Ã  une prÃ©cision de validation d'environ 60-75 % pour ce type de modÃ¨le, mais cela nÃ©cessite un entraÃ®nement sur des images variÃ©es et correctement Ã©tiquetÃ©es.

## ğŸ“ Ã‰motions ciblÃ©es

- ğŸ˜„ joie  
- ğŸ˜¢ tristesse  
- ğŸ˜  colÃ¨re  
- ğŸ˜¨ peur  
- ğŸ˜Œ calme  
- ğŸ˜² Ã©tonnement  

## ğŸ§ª Instructions d'utilisation

1. **Cloner le projet :**

```bash
git clone https://github.com/ton-compte/emotion-classifier.git
cd emotion-classifier
